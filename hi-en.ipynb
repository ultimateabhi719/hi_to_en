{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/administrator/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a5748b87404aa184065a11cdb0a11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['train']['translation'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_corpus(dtype='train', lang='hi'):\n",
    "    l_dataset = len(dataset[dtype])\n",
    "    for i in range(0, l_dataset, 1000):\n",
    "        yield [dataset[dtype][i + j][\"translation\"][lang] for j in range(min(1000,l_dataset-i))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hi_data in training_corpus(dtype='train',lang='hi'):\n",
    "    break;\n",
    "for en_data in training_corpus(dtype='train',lang='en'):\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = old_tokenizer.tokenize(d[0])\n",
    "len(tokens),tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_VOCAB_SIZE = 75000\n",
    "HI_VOCAB_SIZE = 75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus('train'), HI_VOCAB_SIZE)\n",
    "en_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus('train', lang='en'), EN_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer.save_pretrained(\"eng-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_tokenizer.save_pretrained(\"hindi-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = hi_tokenizer.tokenize(hi_data[2])\n",
    "print(len(tokens),tokens)\n",
    "hi_tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = en_tokenizer.tokenize(en_data[2])\n",
    "print(len(tokens),tokens)\n",
    "en_tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizers from saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "hi_tokenizer = AutoTokenizer.from_pretrained(\"hindi-tokenizer\")\n",
    "en_tokenizer = AutoTokenizer.from_pretrained(\"eng-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'cls_token': '<cls>', 'eos_token':'<eos>', 'bos_token' : '<s>'})\n",
    "\n",
    "en_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'cls_token': '<cls>', 'eos_token':'<eos>', 'bos_token' : '<s>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "en_tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=en_tokenizer.bos_token + \" $A \" + en_tokenizer.eos_token,\n",
    "    special_tokens=[(en_tokenizer.eos_token, en_tokenizer.eos_token_id), (en_tokenizer.bos_token, en_tokenizer.bos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sen = dataset['train']['translation'][1]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75003, 60318, 50652, 32916, 75002]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.encode(en_sen, add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 2\n",
    "train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=BS, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset['validation'], batch_size=BS, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=BS, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    break;\n",
    "\n",
    "\n",
    "d = (hi_tokenizer(b['translation']['hi'], padding=True, truncation=True, return_tensors=\"pt\"),\n",
    "en_tokenizer(b['translation']['en'], padding=True, truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=en_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, targets):\n",
    "    \"\"\"Compute our custom loss\"\"\"\n",
    "    predictions = predictions[:, :-1, :].contiguous()\n",
    "    targets = targets[:, 1:]\n",
    "\n",
    "    rearranged_output = predictions.view(predictions.shape[0]*predictions.shape[1], -1)\n",
    "    rearranged_target = targets.contiguous().view(-1)\n",
    "\n",
    "    loss = criterion(rearranged_output, rearranged_target)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "encoder_config = transformers.BertConfig(vocab_size=len(hi_tokenizer))\n",
    "decoder_config = transformers.BertConfig(vocab_size = len(en_tokenizer))\n",
    "\n",
    "config = transformers.EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "model = transformers.EncoderDecoderModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = en_tokenizer.cls_token_id\n",
    "model.config.pad_token_id = en_tokenizer.pad_token_id\n",
    "model.config.eos_token_id = en_tokenizer.eos_token_id\n",
    "model.config.bos_token_id = en_tokenizer.bos_token_id\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/data/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 11.586273193359375\n",
      "epoch 1: 9.458174705505371\n",
      "epoch 2: 8.42220687866211\n",
      "epoch 3: 7.8575568199157715\n",
      "epoch 4: 7.360096454620361\n",
      "epoch 5: 6.655338287353516\n",
      "epoch 6: 6.140342712402344\n",
      "epoch 7: 5.489269733428955\n",
      "epoch 8: 4.9837422370910645\n",
      "epoch 9: 4.242959022521973\n",
      "epoch 10: 3.792346954345703\n",
      "epoch 11: 3.4489777088165283\n",
      "epoch 12: 2.9831364154815674\n",
      "epoch 13: 2.545281171798706\n",
      "epoch 14: 2.3930718898773193\n",
      "epoch 15: 2.0435307025909424\n",
      "epoch 16: 1.7170997858047485\n",
      "epoch 17: 1.5399545431137085\n",
      "epoch 18: 1.369398593902588\n",
      "epoch 19: 1.1758708953857422\n",
      "epoch 20: 1.0006738901138306\n",
      "epoch 21: 0.9694137573242188\n",
      "epoch 22: 0.8083035945892334\n",
      "epoch 23: 0.6873987317085266\n",
      "epoch 24: 0.6448652744293213\n",
      "epoch 25: 0.48635587096214294\n",
      "epoch 26: 0.4270227253437042\n",
      "epoch 27: 0.3203878402709961\n",
      "epoch 28: 0.2857956290245056\n",
      "epoch 29: 0.24199660122394562\n",
      "epoch 30: 0.21654802560806274\n",
      "epoch 31: 0.17343205213546753\n",
      "epoch 32: 0.14722836017608643\n",
      "epoch 33: 0.11802112311124802\n",
      "epoch 34: 0.10346502810716629\n",
      "epoch 35: 0.09153518825769424\n",
      "epoch 36: 0.07757125049829483\n",
      "epoch 37: 0.07082869112491608\n",
      "epoch 38: 0.060067687183618546\n",
      "epoch 39: 0.05145563185214996\n",
      "epoch 40: 0.04612071439623833\n",
      "epoch 41: 0.04144037887454033\n",
      "epoch 42: 0.03710488602519035\n",
      "epoch 43: 0.03489040210843086\n",
      "epoch 44: 0.032010678201913834\n",
      "epoch 45: 0.02855626679956913\n",
      "epoch 46: 0.02610298991203308\n",
      "epoch 47: 0.0248772781342268\n",
      "epoch 48: 0.02262268215417862\n",
      "epoch 49: 0.019870975986123085\n",
      "epoch 50: 0.018792646005749702\n",
      "epoch 51: 0.017651932314038277\n",
      "epoch 52: 0.016901934519410133\n",
      "epoch 53: 0.015888867899775505\n",
      "epoch 54: 0.014068812131881714\n",
      "epoch 55: 0.013621694408357143\n",
      "epoch 56: 0.013324321247637272\n",
      "epoch 57: 0.012546483427286148\n",
      "epoch 58: 0.011900573968887329\n",
      "epoch 59: 0.011079973541200161\n",
      "epoch 60: 0.010936292819678783\n",
      "epoch 61: 0.010370121337473392\n",
      "epoch 62: 0.009896100498735905\n",
      "epoch 63: 0.0094587542116642\n",
      "epoch 64: 0.009094293229281902\n",
      "epoch 65: 0.008720971643924713\n",
      "epoch 66: 0.00869741104543209\n",
      "epoch 67: 0.008243108168244362\n",
      "epoch 68: 0.008214105851948261\n",
      "epoch 69: 0.00788925401866436\n",
      "epoch 70: 0.007528647314757109\n",
      "epoch 71: 0.007421466987580061\n",
      "epoch 72: 0.007040140684694052\n",
      "epoch 73: 0.007161361631006002\n",
      "epoch 74: 0.006828050594776869\n",
      "epoch 75: 0.006636630743741989\n",
      "epoch 76: 0.006598636507987976\n",
      "epoch 77: 0.006342030595988035\n",
      "epoch 78: 0.0062616560608148575\n",
      "epoch 79: 0.006132993381470442\n",
      "epoch 80: 0.006029380485415459\n",
      "epoch 81: 0.005904079880565405\n",
      "epoch 82: 0.005685615818947554\n",
      "epoch 83: 0.005596631672233343\n",
      "epoch 84: 0.005534168798476458\n",
      "epoch 85: 0.00547586428001523\n",
      "epoch 86: 0.005381753668189049\n",
      "epoch 87: 0.00537724606692791\n",
      "epoch 88: 0.005371278151869774\n",
      "epoch 89: 0.005150248296558857\n",
      "epoch 90: 0.00500390725210309\n",
      "epoch 91: 0.005142670590430498\n",
      "epoch 92: 0.004984917584806681\n",
      "epoch 93: 0.004953626077622175\n",
      "epoch 94: 0.0047428118996322155\n",
      "epoch 95: 0.004748198669403791\n",
      "epoch 96: 0.004776541609317064\n",
      "epoch 97: 0.004650180693715811\n",
      "epoch 98: 0.004490205552428961\n",
      "epoch 99: 0.004510033410042524\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    hi_token = d[0]['input_ids'].to(device)\n",
    "    hi_mask = d[0]['attention_mask'].to(device)\n",
    "    en_token = d[1]['input_ids'].to(device)\n",
    "    \n",
    "    out = model(input_ids=hi_token, attention_mask = hi_mask, labels = en_token)[:2]\n",
    "    prediction_scores = out[1]\n",
    "    predictions = F.log_softmax(prediction_scores, dim=-1)\n",
    "    loss = compute_loss(predictions, en_token)\n",
    "\n",
    "    print(f\"epoch {e}:\", loss.item())\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./translate_hin_to_eng.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "encoder_config = transformers.BertConfig(vocab_size=len(hi_tokenizer))\n",
    "decoder_config = transformers.BertConfig(vocab_size = len(en_tokenizer))\n",
    "\n",
    "config = transformers.EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "model = transformers.EncoderDecoderModel(config)\n",
    "\n",
    "model.config.decoder_start_token_id = en_tokenizer.cls_token_id\n",
    "model.config.pad_token_id = en_tokenizer.pad_token_id\n",
    "model.config.eos_token_id = en_tokenizer.eos_token_id\n",
    "model.config.bos_token_id = en_tokenizer.bos_token_id\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(en_tokenizer.decode, d[1]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=d[0]['input_ids'],\n",
    "                     attention_mask = d[0]['attention_mask'],\n",
    "                     labels = d[1]['input_ids']\n",
    "            )\n",
    "\n",
    "list(map(en_tokenizer.decode, torch.argmax(out.logits, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model.generate(input_ids = d[0]['input_ids'], decoder_start_token_id=en_tokenizer.cls_token_id)\n",
    "\n",
    "list(map(en_tokenizer.decode, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# epoch_loss = 0\n",
    "\n",
    "\n",
    "# # optimizer.zero_grad()\n",
    "# out = model(input_ids=d[0]['input_ids'],\n",
    "#                          attention_mask = d[0]['attention_mask'],\n",
    "#                          labels = d[1]['input_ids'])\n",
    "\n",
    "# prediction_scores = out.logits\n",
    "# predictions = F.log_softmax(prediction_scores, dim=-1)\n",
    "# loss = compute_loss(predictions, d[1]['input_ids'])\n",
    "# epoch_loss += loss.item()\n",
    "\n",
    "# print(\"Mean validation loss:\", epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(en_tokenizer.decode, torch.argmax(predictions,dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## greedy decoding\n",
    "BS = 4\n",
    "model.eval()\n",
    "pred_words = torch.tensor([[en_tokenizer.bos_token_id]]*BS)\n",
    "dec_out = pred_words\n",
    "\n",
    "unfinished_seq = np.array([1]*BS)\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    output = model(input_ids = d[0]['input_ids'], labels = dec_out )\n",
    "    pred_words = torch.argmax(output.logits, dim=-1)[:,-1:]    \n",
    "    pred_words[unfinished_seq==0,:] = en_tokenizer.pad_token_id\n",
    "    dec_out = torch.cat((dec_out,pred_words),dim=1)\n",
    "\n",
    "    unfinished_seq[dec_out[:,-1] == en_tokenizer.eos_token_id] = 0\n",
    "\n",
    "list(map(en_tokenizer.decode, dec_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
